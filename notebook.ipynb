{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Projet ML - Bank Marketing: Prediction de Souscription\n\n## 1. Introduction et Contexte\n\n---\n\n## 1.1 Description du Probleme\n\n### Contexte Business\nCe projet porte sur l'analyse de donnees de campagnes de **telemarketing** d'une banque portugaise. L'institution bancaire contacte ses clients par telephone pour leur proposer de souscrire a un **depot a terme** (placement financier).\n\n### Probleme a Resoudre\n**Comment predire si un client va souscrire a un depot a terme suite a un appel telephonique?**\n\nCette prediction permettrait a la banque de:\n- **Optimiser ses ressources**: Cibler les clients les plus susceptibles de souscrire\n- **Reduire les couts**: Eviter les appels inutiles vers des clients non interesses\n- **Ameliorer le taux de conversion**: Concentrer les efforts sur les prospects a fort potentiel\n- **Personnaliser l'approche**: Adapter le discours commercial selon le profil client\n\n### Donnees Disponibles\n- **Source**: UCI Machine Learning Repository\n- **Periode**: Mai 2008 - Novembre 2010\n- **Taille**: 45,211 enregistrements (dataset complet)\n- **Variables**: 16 features + 1 variable cible\n\n---\n\n## 1.2 Classification ou Regression?\n\n### Analyse du Type de Probleme\n\n| Aspect | Classification | Regression |\n|--------|---------------|------------|\n| **Variable cible** | Categorique (classes) | Continue (valeurs numeriques) |\n| **Objectif** | Predire une categorie | Predire une valeur |\n| **Exemple** | Oui/Non, Chat/Chien | Prix, Temperature |\n\n### Notre Cas: **CLASSIFICATION BINAIRE**\n\nLa variable cible `y` represente la reponse du client:\n- **`yes`** (classe 1): Le client a souscrit au depot a terme\n- **`no`** (classe 0): Le client n'a pas souscrit\n\n**Pourquoi ce n'est PAS une regression?**\n- La variable cible n'est pas numerique continue\n- On ne predit pas un montant ou une probabilite brute\n- On cherche a **classifier** les clients en deux groupes distincts\n\n**Pourrait-on traiter ce probleme autrement?**\n- **Regression logistique**: Techniquement une regression, mais utilisee pour la classification (predit une probabilite puis applique un seuil)\n- **Regression de probabilite**: On pourrait predire la probabilite de souscription (valeur entre 0 et 1), mais l'objectif final reste une decision binaire\n\n**Conclusion**: Ce projet est traite comme un probleme de **classification binaire supervisee**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Exploration des Donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import des librairies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix, classification_report,\n    precision_recall_curve\n)\n\n# SMOTE pour le surechantillonnage\nfrom imblearn.over_sampling import SMOTE\n\n# Configuration\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-whitegrid')\n%matplotlib inline\n\n# Seed pour reproductibilite\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donnees (petit dataset pour les tests)\n",
    "df = pd.read_csv('data/bank.csv', sep=';')\n",
    "\n",
    "print(f\"Dimensions du dataset: {df.shape}\")\n",
    "print(f\"Nombre d'exemples: {df.shape[0]}\")\n",
    "print(f\"Nombre de features: {df.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apercu des premieres lignes\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur les types de donnees\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives des variables numeriques\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques des variables categorielles\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Definition des Features et de la Variable Cible\n\n---\n\n### Variable Cible (Target)\n\n| Variable | Type | Valeurs | Description |\n|----------|------|---------|-------------|\n| **`y`** | Binaire | `yes` / `no` | Le client a-t-il souscrit au depot a terme? |\n\nC'est la variable que nous cherchons a **predire**. Elle sera encodee en:\n- `1` = souscription (yes)\n- `0` = pas de souscription (no)\n\n---\n\n### Features (Variables Explicatives)\n\nLes 16 features sont reparties en 4 categories:\n\n#### A. Donnees Demographiques du Client (8 variables)\n| Variable | Type | Description | Valeurs Possibles |\n|----------|------|-------------|-------------------|\n| `age` | Numerique | Age du client | 18-95 |\n| `job` | Categorique | Type d'emploi | admin, blue-collar, entrepreneur, housemaid, management, retired, self-employed, services, student, technician, unemployed, unknown |\n| `marital` | Categorique | Statut marital | married, divorced, single |\n| `education` | Categorique | Niveau d'education | primary, secondary, tertiary, unknown |\n| `default` | Binaire | Credit en defaut? | yes, no |\n| `balance` | Numerique | Solde annuel moyen (euros) | -8019 a 102127 |\n| `housing` | Binaire | Pret immobilier en cours? | yes, no |\n| `loan` | Binaire | Pret personnel en cours? | yes, no |\n\n#### B. Donnees du Dernier Contact - Campagne Actuelle (4 variables)\n| Variable | Type | Description | Note |\n|----------|------|-------------|------|\n| `contact` | Categorique | Type de communication | cellular, telephone, unknown |\n| `day` | Numerique | Jour du mois du dernier contact | 1-31 |\n| `month` | Categorique | Mois du dernier contact | jan-dec |\n| `duration` | Numerique | Duree du dernier appel (secondes) | **EXCLUE - Data Leakage** |\n\n> **ATTENTION**: La variable `duration` est connue uniquement APRES l'appel. L'utiliser pour predire le resultat de l'appel constitue du **data leakage** (fuite de donnees). Elle sera exclue du modele.\n\n#### C. Historique des Campagnes (4 variables)\n| Variable | Type | Description | Valeurs Particulieres |\n|----------|------|-------------|----------------------|\n| `campaign` | Numerique | Nombre de contacts pendant cette campagne | Min: 1 |\n| `pdays` | Numerique | Jours depuis le dernier contact (campagne precedente) | -1 = jamais contacte |\n| `previous` | Numerique | Nombre de contacts avant cette campagne | 0 = premiere campagne |\n| `poutcome` | Categorique | Resultat de la campagne precedente | success, failure, other, unknown |\n\n---\n\n### Resume des Features\n\n| Categorie | Nombre | Variables |\n|-----------|--------|-----------|\n| Numeriques | 6 | age, balance, day, campaign, pdays, previous |\n| Categorielles | 9 | job, marital, education, default, housing, loan, contact, month, poutcome |\n| **Exclue** | 1 | duration (data leakage) |\n| **Total utilisees** | **15** | |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyse de la Variable Cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "target_counts = df['y'].value_counts()\n",
    "target_pct = df['y'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Distribution de la variable cible:\")\n",
    "print(target_counts)\n",
    "print(f\"\\nPourcentages:\")\n",
    "print(f\"  No:  {target_pct['no']:.1f}%\")\n",
    "print(f\"  Yes: {target_pct['yes']:.1f}%\")\n",
    "print(f\"\\nRatio de desequilibre: {target_counts['no'] / target_counts['yes']:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Barplot\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(target_counts.index, target_counts.values, color=colors)\n",
    "ax1.set_title('Distribution de la Variable Cible', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Souscription')\n",
    "ax1.set_ylabel('Nombre de clients')\n",
    "for bar, count in zip(bars, target_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "             str(count), ha='center', fontsize=11)\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie(target_counts.values, labels=['Non', 'Oui'], autopct='%1.1f%%', \n",
    "        colors=colors, explode=[0, 0.05], startangle=90)\n",
    "ax2.set_title('Proportion des Classes', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=> Le dataset est DESEQUILIBRE. La classe 'no' est majoritaire.\")\n",
    "print(\"   Cela devra etre pris en compte lors de la modelisation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyse des Variables Numeriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numeriques (sans duration)\n",
    "numeric_cols = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramme par classe\n",
    "    for label, color in [('no', '#e74c3c'), ('yes', '#2ecc71')]:\n",
    "        data = df[df['y'] == label][col]\n",
    "        ax.hist(data, bins=30, alpha=0.6, label=label, color=color, density=True)\n",
    "    \n",
    "    ax.set_title(f'Distribution de {col}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Densite')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots par classe\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    df.boxplot(column=col, by='y', ax=ax)\n",
    "    ax.set_title(f'{col} par classe', fontsize=11)\n",
    "    ax.set_xlabel('Souscription')\n",
    "    ax.set_ylabel(col)\n",
    "\n",
    "plt.suptitle('Boxplots des Variables Numeriques par Classe', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analyse des Variables Categorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categorielles\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Afficher les valeurs uniques\n",
    "print(\"Valeurs uniques par variable categorielle:\\n\")\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de souscription par categorie pour les principales variables\n",
    "main_cat_cols = ['job', 'marital', 'education', 'contact', 'poutcome']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(main_cat_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Calculer le taux de souscription\n",
    "    rate = df.groupby(col)['y'].apply(lambda x: (x == 'yes').mean() * 100).sort_values(ascending=False)\n",
    "    \n",
    "    bars = ax.barh(range(len(rate)), rate.values, color='steelblue')\n",
    "    ax.set_yticks(range(len(rate)))\n",
    "    ax.set_yticklabels(rate.index)\n",
    "    ax.set_xlabel('Taux de souscription (%)')\n",
    "    ax.set_title(f'Taux de souscription par {col}', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for bar, val in zip(bars, rate.values):\n",
    "        ax.text(val + 0.5, bar.get_y() + bar.get_height()/2, f'{val:.1f}%', va='center')\n",
    "\n",
    "# Cacher le dernier subplot vide\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du mois de contact\n",
    "month_order = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "month_rate = df.groupby('month')['y'].apply(lambda x: (x == 'yes').mean() * 100)\n",
    "month_rate = month_rate.reindex(month_order)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.bar(month_rate.index, month_rate.values, color='steelblue')\n",
    "plt.title('Taux de Souscription par Mois', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Mois')\n",
    "plt.ylabel('Taux de souscription (%)')\n",
    "plt.axhline(y=df['y'].apply(lambda x: x == 'yes').mean() * 100, \n",
    "            color='red', linestyle='--', label='Moyenne globale')\n",
    "plt.legend()\n",
    "\n",
    "for bar, val in zip(bars, month_rate.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{val:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Analyse des Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder temporairement la variable cible pour la correlation\n",
    "df_corr = df.copy()\n",
    "df_corr['y_encoded'] = (df_corr['y'] == 'yes').astype(int)\n",
    "\n",
    "# Matrice de correlation des variables numeriques\n",
    "numeric_for_corr = numeric_cols + ['y_encoded']\n",
    "corr_matrix = df_corr[numeric_for_corr].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Matrice de Correlation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations avec la variable cible (y_encoded):\")\n",
    "print(corr_matrix['y_encoded'].drop('y_encoded').sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Valeurs Manquantes et \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier les valeurs manquantes\n",
    "print(\"Valeurs manquantes par colonne:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal: {df.isnull().sum().sum()} valeurs manquantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier les valeurs \"unknown\"\n",
    "print(\"Valeurs 'unknown' par colonne:\\n\")\n",
    "for col in cat_cols:\n",
    "    unknown_count = (df[col] == 'unknown').sum()\n",
    "    if unknown_count > 0:\n",
    "        pct = unknown_count / len(df) * 100\n",
    "        print(f\"{col}: {unknown_count} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n=> Les valeurs 'unknown' seront traitees comme une categorie a part entiere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Observations Cles de l'EDA\n",
    "\n",
    "**1. Desequilibre des classes:**\n",
    "- La classe \"no\" est largement majoritaire (~88%)\n",
    "- Necessite d'utiliser des techniques adaptees (class_weight, SMOTE, etc.)\n",
    "\n",
    "**2. Variables importantes:**\n",
    "- `poutcome = 'success'` a un tres fort taux de souscription\n",
    "- Les etudiants et retraites souscrivent plus souvent\n",
    "- Les mois de mars, septembre, octobre et decembre ont de meilleurs taux\n",
    "\n",
    "**3. Valeurs \"unknown\":**\n",
    "- Presentes dans `job`, `education`, `contact`, et `poutcome`\n",
    "- Seront conservees comme categorie separee\n",
    "\n",
    "**4. Variable `pdays`:**\n",
    "- -1 signifie \"jamais contacte\" (majorite des cas)\n",
    "- Distribution tres asymetrique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing des Donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie du dataframe pour le preprocessing\n",
    "df_prep = df.copy()\n",
    "\n",
    "# Supprimer la colonne 'duration' (data leakage)\n",
    "df_prep = df_prep.drop('duration', axis=1)\n",
    "print(\"Colonne 'duration' supprimee (data leakage).\")\n",
    "print(f\"Dimensions apres suppression: {df_prep.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder la variable cible\n",
    "df_prep['y'] = (df_prep['y'] == 'yes').astype(int)\n",
    "print(f\"Variable cible encodee: 'yes' -> 1, 'no' -> 0\")\n",
    "print(f\"Distribution: {df_prep['y'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes\n",
    "target = 'y'\n",
    "numeric_features = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "print(f\"Features numeriques ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Features categorielles ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding des variables categorielles\n",
    "df_encoded = pd.get_dummies(df_prep, columns=categorical_features, drop_first=False)\n",
    "\n",
    "print(f\"Dimensions apres One-Hot Encoding: {df_encoded.shape}\")\n",
    "print(f\"Nombre de features: {df_encoded.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separer features et target\n",
    "X = df_encoded.drop(target, axis=1)\n",
    "y = df_encoded[target]\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (80/20, stratifie)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} exemples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set:  {X_test.shape[0]} exemples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nDistribution dans train: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Distribution dans test:  {y_test.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des features numeriques\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit sur train, transform sur train et test\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "print(\"Standardisation appliquee aux variables numeriques.\")\n",
    "print(f\"\\nMoyennes apres scaling (train): {X_train_scaled[numeric_features].mean().round(2).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 SMOTE - Surechantillonnage de la Classe Minoritaire\n\nLe dataset est fortement desequilibre (~88% de \"no\"). Pour ameliorer le recall, nous utilisons SMOTE (Synthetic Minority Over-sampling Technique) pour generer des exemples synthetiques de la classe minoritaire.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Appliquer SMOTE sur les donnees d'entrainement\nprint(\"Application de SMOTE sur les donnees d'entrainement...\")\nprint(f\"\\nAvant SMOTE:\")\nprint(f\"  Classe 0 (no):  {(y_train == 0).sum()}\")\nprint(f\"  Classe 1 (yes): {(y_train == 1).sum()}\")\n\nsmote = SMOTE(random_state=RANDOM_STATE)\n\n# SMOTE sur donnees non scalees (pour RF, GB)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# SMOTE sur donnees scalees (pour LR, SVM, KNN)\nX_train_scaled_smote, y_train_scaled_smote = smote.fit_resample(X_train_scaled, y_train)\n\nprint(f\"\\nApres SMOTE:\")\nprint(f\"  Classe 0 (no):  {(y_train_smote == 0).sum()}\")\nprint(f\"  Classe 1 (yes): {(y_train_smote == 1).sum()}\")\nprint(f\"\\n=> Les classes sont maintenant equilibrees!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Modelisation\n\n---\n\n## 4.1 Choix et Justification des Algorithmes\n\nNous utilisons **5 algorithmes differents** pour comparer leurs performances. Ce choix repond a l'exigence d'utiliser **plusieurs types de modeles**.\n\n### Tableau Comparatif des Algorithmes\n\n| # | Algorithme | Type | Famille | Pourquoi ce choix? |\n|---|------------|------|---------|-------------------|\n| 1 | **Logistic Regression** | Lineaire | Baseline | Modele simple et interpretable, sert de reference pour comparer les autres modeles |\n| 2 | **Random Forest** | Ensemble | Bagging | Robuste, gere bien les features mixtes, resistant au sur-apprentissage |\n| 3 | **Gradient Boosting** | Ensemble | Boosting | Tres performant sur les donnees tabulaires, apprend des erreurs successives |\n| 4 | **SVM (RBF)** | Kernel | Marge | Efficace pour les frontieres de decision non-lineaires |\n| 5 | **KNN** | Instance | Voisinage | Simple, non-parametrique, utile comme comparaison |\n\n---\n\n### Detail des Algorithmes\n\n#### 1. Logistic Regression (Modele de Base)\n- **Principe**: Modele lineaire qui predit la probabilite d'appartenance a une classe\n- **Avantages**: Interpretable, rapide, donne des probabilites calibrees\n- **Inconvenients**: Suppose une relation lineaire entre features et log-odds\n- **Utilisation**: Sert de **baseline** pour evaluer si les modeles plus complexes apportent une amelioration\n\n#### 2. Random Forest (Ensemble - Bagging)\n- **Principe**: Combine plusieurs arbres de decision entraines sur des echantillons bootstrap\n- **Avantages**: Resistant au sur-apprentissage, gere les features categorielles, fournit l'importance des variables\n- **Inconvenients**: Moins interpretable, peut etre lent sur de gros datasets\n- **Parametres cles**: `n_estimators` (nombre d'arbres), `max_depth` (profondeur)\n\n#### 3. Gradient Boosting (Ensemble - Boosting)\n- **Principe**: Construit des arbres de facon sequentielle, chaque arbre corrige les erreurs du precedent\n- **Avantages**: Excellentes performances sur les donnees tabulaires, flexible\n- **Inconvenients**: Risque de sur-apprentissage si mal regularise, plus lent que RF\n- **Parametres cles**: `learning_rate`, `n_estimators`, `max_depth`\n\n#### 4. Support Vector Machine (SVM avec kernel RBF)\n- **Principe**: Trouve l'hyperplan optimal qui separe les classes avec une marge maximale\n- **Avantages**: Efficace en haute dimension, kernel RBF pour relations non-lineaires\n- **Inconvenients**: Sensible au scaling, lent sur grands datasets, moins interpretable\n- **Note**: Necessite la standardisation des donnees\n\n#### 5. K-Nearest Neighbors (KNN)\n- **Principe**: Classe un point selon la majorite de ses k plus proches voisins\n- **Avantages**: Simple, non-parametrique, pas d'entrainement\n- **Inconvenients**: Lent en prediction sur grands datasets, sensible au scaling et au choix de k\n- **Parametres cles**: `n_neighbors` (nombre de voisins), `weights` (uniform ou distance)\n\n---\n\n### Diversite des Approches\n\n| Approche | Algorithmes |\n|----------|-------------|\n| **Lineaire** | Logistic Regression |\n| **Non-lineaire** | SVM (RBF), KNN |\n| **Ensemble - Bagging** | Random Forest |\n| **Ensemble - Boosting** | Gradient Boosting |\n\nCette diversite permet de:\n1. Comparer des approches fondamentalement differentes\n2. Identifier le type de modele le plus adapte aux donnees\n3. Avoir une vision complete des performances possibles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour evaluer un modele\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Entraine un modele et retourne ses metriques de performance.\n",
    "    \"\"\"\n",
    "    # Entrainement\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Metriques\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'AUC-ROC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher la matrice de confusion\n",
    "def plot_confusion_matrix(y_test, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de confusion.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No (0)', 'Yes (1)'],\n",
    "                yticklabels=['No (0)', 'Yes (1)'])\n",
    "    plt.title(f'Matrice de Confusion - {model_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Reel')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpretation\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"Vrais Negatifs (TN): {tn}\")\n",
    "    print(f\"Faux Positifs (FP): {fp}\")\n",
    "    print(f\"Faux Negatifs (FN): {fn}\")\n",
    "    print(f\"Vrais Positifs (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les resultats\n",
    "results = []\n",
    "predictions = {}\n",
    "probabilities = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Logistic Regression avec SMOTE\nlr_model = LogisticRegression(\n    max_iter=1000,\n    random_state=RANDOM_STATE\n)\n\nprint(\"=\" * 50)\nprint(\"LOGISTIC REGRESSION (avec SMOTE)\")\nprint(\"=\" * 50)\n\nlr_metrics, lr_pred, lr_proba = evaluate_model(\n    lr_model, X_train_scaled_smote, X_test_scaled, y_train_scaled_smote, y_test, 'Logistic Regression'\n)\n\nresults.append(lr_metrics)\npredictions['Logistic Regression'] = lr_pred\nprobabilities['Logistic Regression'] = lr_proba\n\nprint(f\"\\nResultats:\")\nfor k, v in lr_metrics.items():\n    if k != 'Model' and v is not None:\n        print(f\"  {k}: {v:.4f}\")\n\nplot_confusion_matrix(y_test, lr_pred, 'Logistic Regression')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Random Forest avec SMOTE\nrf_model = RandomForestClassifier(\n    n_estimators=100,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nprint(\"=\" * 50)\nprint(\"RANDOM FOREST (avec SMOTE)\")\nprint(\"=\" * 50)\n\nrf_metrics, rf_pred, rf_proba = evaluate_model(\n    rf_model, X_train_smote, X_test, y_train_smote, y_test, 'Random Forest'\n)\n\nresults.append(rf_metrics)\npredictions['Random Forest'] = rf_pred\nprobabilities['Random Forest'] = rf_proba\n\nprint(f\"\\nResultats:\")\nfor k, v in rf_metrics.items():\n    if k != 'Model' and v is not None:\n        print(f\"  {k}: {v:.4f}\")\n\nplot_confusion_matrix(y_test, rf_pred, 'Random Forest')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gradient Boosting avec SMOTE\ngb_model = GradientBoostingClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=RANDOM_STATE\n)\n\nprint(\"=\" * 50)\nprint(\"GRADIENT BOOSTING (avec SMOTE)\")\nprint(\"=\" * 50)\n\ngb_metrics, gb_pred, gb_proba = evaluate_model(\n    gb_model, X_train_smote, X_test, y_train_smote, y_test, 'Gradient Boosting'\n)\n\nresults.append(gb_metrics)\npredictions['Gradient Boosting'] = gb_pred\nprobabilities['Gradient Boosting'] = gb_proba\n\nprint(f\"\\nResultats:\")\nfor k, v in gb_metrics.items():\n    if k != 'Model' and v is not None:\n        print(f\"  {k}: {v:.4f}\")\n\nplot_confusion_matrix(y_test, gb_pred, 'Gradient Boosting')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SVM avec SMOTE\nsvm_model = SVC(\n    kernel='rbf',\n    probability=True,\n    random_state=RANDOM_STATE\n)\n\nprint(\"=\" * 50)\nprint(\"SUPPORT VECTOR MACHINE (avec SMOTE)\")\nprint(\"=\" * 50)\n\nsvm_metrics, svm_pred, svm_proba = evaluate_model(\n    svm_model, X_train_scaled_smote, X_test_scaled, y_train_scaled_smote, y_test, 'SVM'\n)\n\nresults.append(svm_metrics)\npredictions['SVM'] = svm_pred\nprobabilities['SVM'] = svm_proba\n\nprint(f\"\\nResultats:\")\nfor k, v in svm_metrics.items():\n    if k != 'Model' and v is not None:\n        print(f\"  {k}: {v:.4f}\")\n\nplot_confusion_matrix(y_test, svm_pred, 'SVM')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# KNN avec SMOTE\nknn_model = KNeighborsClassifier(\n    n_neighbors=5,\n    weights='distance',\n    n_jobs=-1\n)\n\nprint(\"=\" * 50)\nprint(\"K-NEAREST NEIGHBORS (avec SMOTE)\")\nprint(\"=\" * 50)\n\nknn_metrics, knn_pred, knn_proba = evaluate_model(\n    knn_model, X_train_scaled_smote, X_test_scaled, y_train_scaled_smote, y_test, 'KNN'\n)\n\nresults.append(knn_metrics)\npredictions['KNN'] = knn_pred\nprobabilities['KNN'] = knn_proba\n\nprint(f\"\\nResultats:\")\nfor k, v in knn_metrics.items():\n    if k != 'Model' and v is not None:\n        print(f\"  {k}: {v:.4f}\")\n\nplot_confusion_matrix(y_test, knn_pred, 'KNN')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Evaluation et Comparaison des Modeles\n\n---\n\n## 5.1 Choix et Justification des Metriques\n\nPour un probleme de classification binaire **desequilibre**, le choix des metriques est crucial. Voici les metriques utilisees et leur justification:\n\n### Tableau des Metriques\n\n| Metrique | Formule | Description | Quand l'utiliser? |\n|----------|---------|-------------|-------------------|\n| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Taux de predictions correctes | Donnees equilibrees |\n| **Precision** | TP/(TP+FP) | Parmi les positifs predits, combien sont vrais? | Cout eleve des faux positifs |\n| **Recall (Sensibilite)** | TP/(TP+FN) | Parmi les vrais positifs, combien sont detectes? | Cout eleve des faux negatifs |\n| **F1-Score** | 2*(Precision*Recall)/(Precision+Recall) | Moyenne harmonique precision/recall | Compromis entre precision et recall |\n| **AUC-ROC** | Aire sous la courbe ROC | Capacite a discriminer les classes | Comparaison independante du seuil |\n\n### Interpretation dans notre Contexte Business\n\n| Erreur | Signification | Consequence |\n|--------|---------------|-------------|\n| **Faux Positif (FP)** | Client predit \"souscrit\" mais ne souscrit pas | Appel inutile (cout operationnel) |\n| **Faux Negatif (FN)** | Client predit \"ne souscrit pas\" mais aurait souscrit | **Opportunite manquee (perte de revenus)** |\n\n### Pourquoi le Recall est Important?\n\nDans le contexte bancaire:\n- Un **faux negatif** = un client potentiel NON contacte = **perte de revenus**\n- Un **faux positif** = un appel supplementaire = cout relativement faible\n\n**Conclusion**: Le **Recall** est particulierement important car on ne veut pas rater de clients potentiels. C'est pourquoi nous utilisons SMOTE et optimisons le recall.\n\n### Pourquoi l'Accuracy n'est PAS Suffisante?\n\nAvec un dataset desequilibre (88% de \"no\"):\n- Un modele qui predit TOUJOURS \"no\" aurait 88% d'accuracy!\n- Mais il aurait 0% de recall (ne detecte aucun client potentiel)\n\n**L'accuracy seule est trompeuse pour les classes desequilibrees.**\n\n### Metrique Principale Choisie\n\n**F1-Score** comme compromis, mais avec attention particuliere au **Recall** pour minimiser les opportunites manquees."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif des resultats\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "# Formater pour l'affichage\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON DES MODELES\")\n",
    "print(\"=\" * 70)\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des metriques\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(results_df.index))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = results_df[metric].values\n",
    "    bars = ax.bar(x + i * width, values, width, label=metric)\n",
    "\n",
    "ax.set_xlabel('Modele')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des Metriques par Modele', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(results_df.index, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "model_names = list(probabilities.keys())\n",
    "\n",
    "for i, (name, proba) in enumerate(probabilities.items()):\n",
    "    if proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "        auc = roc_auc_score(y_test, proba)\n",
    "        plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "                 label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "# Ligne de reference (classificateur aleatoire)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Random')\n",
    "\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)\n",
    "plt.title('Courbes ROC - Comparaison des Modeles', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier le meilleur modele selon differentes metriques\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MEILLEUR MODELE PAR METRIQUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    best_model = results_df[metric].idxmax()\n",
    "    best_score = results_df[metric].max()\n",
    "    print(f\"{metric:12s}: {best_model:20s} ({best_score:.4f})\")\n",
    "\n",
    "# Meilleur modele global (F1-Score comme critere principal pour classes desequilibrees)\n",
    "best_overall = results_df['F1-Score'].idxmax()\n",
    "print(f\"\\n=> Meilleur modele global (F1-Score): {best_overall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance du Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "top_20 = feature_importance.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_20)), top_20['importance'].values, color='steelblue')\n",
    "plt.yticks(range(len(top_20)), top_20['feature'].values)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Features - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features les plus importantes:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Optimisation et Phase de Validation\n\n---\n\n## 6.1 Strategie de Validation\n\nLa validation est cruciale pour s'assurer que notre modele **generalise bien** sur de nouvelles donnees et n'est pas en sur-apprentissage.\n\n### Techniques de Validation Utilisees\n\n| Technique | Description | Objectif |\n|-----------|-------------|----------|\n| **Train/Test Split** | 80% train, 20% test | Evaluation finale non biaisee |\n| **Stratified Split** | Conservation des proportions de classes | Eviter le biais du desequilibre |\n| **K-Fold Cross-Validation** | 5 folds avec rotation | Estimation robuste des performances |\n| **GridSearchCV** | Recherche exhaustive des hyperparametres | Optimisation du modele |\n\n---\n\n### 1. Train/Test Split (80/20)\n\n```\nDataset complet (100%)\n    |\n    +---> Train Set (80%) ---> Entrainement des modeles\n    |                              |\n    |                              +---> Cross-Validation (5-Fold)\n    |                                        |\n    |                                        +---> Fold 1: Train(80%) / Val(20%)\n    |                                        +---> Fold 2: Train(80%) / Val(20%)\n    |                                        +---> Fold 3: Train(80%) / Val(20%)\n    |                                        +---> Fold 4: Train(80%) / Val(20%)\n    |                                        +---> Fold 5: Train(80%) / Val(20%)\n    |\n    +---> Test Set (20%) ---> Evaluation finale (jamais vu pendant l'entrainement)\n```\n\n**Pourquoi 80/20?**\n- 80% assure suffisamment de donnees pour l'entrainement\n- 20% permet une evaluation statistiquement significative\n- Avec 45,211 exemples: ~9,000 exemples pour le test\n\n---\n\n### 2. Stratified Split\n\nLe split est **stratifie** pour conserver les proportions de la variable cible:\n\n| Ensemble | Classe 0 (no) | Classe 1 (yes) | Ratio |\n|----------|---------------|----------------|-------|\n| Dataset complet | 88.5% | 11.5% | 7.7:1 |\n| Train set | 88.5% | 11.5% | 7.7:1 |\n| Test set | 88.5% | 11.5% | 7.7:1 |\n\n**Sans stratification**, le test set pourrait avoir une distribution differente, faussant l'evaluation.\n\n---\n\n### 3. K-Fold Cross-Validation (K=5)\n\nPendant l'optimisation, nous utilisons la **validation croisee stratifiee**:\n\n- Le train set est divise en 5 \"folds\" (parties)\n- A chaque iteration, 4 folds servent a l'entrainement, 1 fold a la validation\n- On repete 5 fois en changeant le fold de validation\n- Le score final = moyenne des 5 scores\n\n**Avantages:**\n- Utilise toutes les donnees pour l'entrainement ET la validation\n- Estimation plus robuste des performances\n- Detecte le sur-apprentissage\n\n---\n\n### 4. GridSearchCV pour l'Optimisation\n\nL'optimisation des hyperparametres utilise `GridSearchCV`:\n\n```python\nGridSearchCV(\n    estimator=model,\n    param_grid={...},\n    cv=StratifiedKFold(n_splits=5),\n    scoring='recall'  # Optimise pour le recall\n)\n```\n\n**Processus:**\n1. Definir une grille de parametres a tester\n2. Pour chaque combinaison de parametres:\n   - Entrainer avec cross-validation 5-fold\n   - Calculer le score moyen\n3. Selectionner la combinaison avec le meilleur score\n4. Reentrainer sur tout le train set avec ces parametres\n\n---\n\n### Resume de la Strategie de Validation\n\n| Etape | Methode | Donnees Utilisees |\n|-------|---------|-------------------|\n| Comparaison des modeles | Train/Test split (80/20) stratifie | bank.csv (4,521 ex.) |\n| Optimisation hyperparametres | GridSearchCV avec 5-Fold CV | Train set |\n| Evaluation finale | Prediction sur test set | Test set (non vu) |\n| Resultats finaux | Re-entrainement complet | bank-full.csv (45,211 ex.) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimisation de Gradient Boosting avec SMOTE + GridSearchCV\nprint(\"Optimisation des hyperparametres de Gradient Boosting (avec SMOTE)...\")\nprint(\"(Cela peut prendre quelques minutes)\\n\")\n\n# Grille de parametres\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Cross-validation stratifiee\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\n# GridSearchCV sur donnees SMOTE\ngb_opt = GradientBoostingClassifier(random_state=RANDOM_STATE)\n\ngrid_search = GridSearchCV(\n    gb_opt, param_grid, \n    cv=cv, \n    scoring='recall',  # Optimiser pour le recall\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train_smote, y_train_smote)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleurs parametres\n",
    "print(\"Meilleurs hyperparametres:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMeilleur score F1 (CV): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer le modele optimise sur le test set\n",
    "best_gb = grid_search.best_estimator_\n",
    "\n",
    "y_pred_opt = best_gb.predict(X_test)\n",
    "y_proba_opt = best_gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESULTATS DU MODELE OPTIMISE (Gradient Boosting)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_opt):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_opt):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_opt):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_opt):.4f}\")\n",
    "print(f\"AUC-ROC:   {roc_auc_score(y_test, y_proba_opt):.4f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "plot_confusion_matrix(y_test, y_pred_opt, 'Gradient Boosting Optimise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification complet\n",
    "print(\"\\nRapport de classification detaille:\")\n",
    "print(classification_report(y_test, y_pred_opt, target_names=['No', 'Yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 6.1 Optimisation du Seuil de Decision\n\nLe seuil par defaut est 0.5, mais on peut l'ajuster pour maximiser le recall au detriment de la precision. Trouvons le seuil optimal.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Trouver le seuil optimal pour maximiser le recall\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_opt)\n\n# Visualisation Precision-Recall vs Seuil\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe Precision-Recall\nax1 = axes[0]\nax1.plot(recalls, precisions, color='blue', lw=2)\nax1.set_xlabel('Recall', fontsize=12)\nax1.set_ylabel('Precision', fontsize=12)\nax1.set_title('Courbe Precision-Recall', fontsize=14, fontweight='bold')\nax1.grid(alpha=0.3)\n\n# Precision et Recall vs Seuil\nax2 = axes[1]\nax2.plot(thresholds, precisions[:-1], label='Precision', color='blue', lw=2)\nax2.plot(thresholds, recalls[:-1], label='Recall', color='green', lw=2)\nax2.axvline(x=0.5, color='red', linestyle='--', label='Seuil par defaut (0.5)')\nax2.set_xlabel('Seuil de Decision', fontsize=12)\nax2.set_ylabel('Score', fontsize=12)\nax2.set_title('Precision et Recall vs Seuil', fontsize=14, fontweight='bold')\nax2.legend()\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Trouver le seuil qui donne recall >= 0.6 avec la meilleure precision\ntarget_recall = 0.6\nvalid_indices = recalls[:-1] >= target_recall\nif valid_indices.any():\n    best_idx = np.argmax(precisions[:-1][valid_indices])\n    best_threshold = thresholds[valid_indices][best_idx]\nelse:\n    best_threshold = 0.3  # Seuil par defaut si target_recall non atteint\n\nprint(f\"Seuil optimal pour Recall >= {target_recall}: {best_threshold:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resultats Finaux sur le Dataset Complet\n",
    "\n",
    "Nous allons maintenant entrainer notre meilleur modele sur le dataset complet (`bank-full.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset complet\n",
    "print(\"Chargement du dataset complet (bank-full.csv)...\")\n",
    "df_full = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "print(f\"Dimensions: {df_full.shape}\")\n",
    "print(f\"Distribution de y: {df_full['y'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing du dataset complet (meme pipeline)\n",
    "df_full_prep = df_full.copy()\n",
    "\n",
    "# Supprimer duration\n",
    "df_full_prep = df_full_prep.drop('duration', axis=1)\n",
    "\n",
    "# Encoder la variable cible\n",
    "df_full_prep['y'] = (df_full_prep['y'] == 'yes').astype(int)\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_full_encoded = pd.get_dummies(df_full_prep, columns=categorical_features, drop_first=False)\n",
    "\n",
    "# Separer features et target\n",
    "X_full = df_full_encoded.drop('y', axis=1)\n",
    "y_full = df_full_encoded['y']\n",
    "\n",
    "print(f\"Features: {X_full.shape[1]}\")\n",
    "print(f\"Exemples: {X_full.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=RANDOM_STATE, stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train_full.shape[0]} exemples\")\n",
    "print(f\"Test:  {X_test_full.shape[0]} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Appliquer SMOTE sur le dataset complet\nsmote_full = SMOTE(random_state=RANDOM_STATE)\nX_train_full_smote, y_train_full_smote = smote_full.fit_resample(X_train_full, y_train_full)\n\nprint(f\"Apres SMOTE sur dataset complet:\")\nprint(f\"  Classe 0: {(y_train_full_smote == 0).sum()}\")\nprint(f\"  Classe 1: {(y_train_full_smote == 1).sum()}\")\n\n# Entrainer le modele optimise sur le dataset complet avec SMOTE\nfinal_model = GradientBoostingClassifier(\n    **grid_search.best_params_,\n    random_state=RANDOM_STATE\n)\n\nprint(\"\\nEntrainement du modele final sur le dataset complet (avec SMOTE)...\")\nfinal_model.fit(X_train_full_smote, y_train_full_smote)\nprint(\"Entrainement termine.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation finale\n",
    "y_pred_final = final_model.predict(X_test_full)\n",
    "y_proba_final = final_model.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTATS FINAUX - DATASET COMPLET (45,211 exemples)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModele: Gradient Boosting avec hyperparametres optimises\")\n",
    "print(f\"\\nMetriques sur le test set ({len(y_test_full)} exemples):\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test_full, y_pred_final):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test_full, y_pred_final):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test_full, y_pred_final):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_test_full, y_pred_final):.4f}\")\n",
    "print(f\"  AUC-ROC:   {roc_auc_score(y_test_full, y_proba_final):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion finale\n",
    "plot_confusion_matrix(y_test_full, y_pred_final, 'Gradient Boosting Final (Dataset Complet)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification final\n",
    "print(\"\\nRapport de classification final:\")\n",
    "print(classification_report(y_test_full, y_pred_final, target_names=['No', 'Yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe ROC finale\n",
    "fpr_final, tpr_final, thresholds = roc_curve(y_test_full, y_proba_final)\n",
    "auc_final = roc_auc_score(y_test_full, y_proba_final)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_final, tpr_final, color='blue', lw=2, label=f'Gradient Boosting (AUC = {auc_final:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Random')\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)\n",
    "plt.title('Courbe ROC - Modele Final', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance finale\n",
    "feature_importance_final = pd.DataFrame({\n",
    "    'feature': X_train_full.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_15 = feature_importance_final.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_15)), top_15['importance'].values, color='steelblue')\n",
    "plt.yticks(range(len(top_15)), top_15['feature'].values)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Features - Modele Final', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Conclusion\n\n---\n\n### 8.1 Resume du Projet\n\n| Aspect | Description |\n|--------|-------------|\n| **Probleme** | Predire si un client va souscrire a un depot a terme (classification binaire) |\n| **Donnees** | 45,211 exemples, 15 features (sans `duration`) |\n| **Desequilibre** | 88.5% \"no\" vs 11.5% \"yes\" (ratio 7.7:1) |\n| **Solution** | SMOTE + Gradient Boosting optimise |\n\n---\n\n### 8.2 Synthese des Algorithmes Testes\n\n| Algorithme | Type | Forces | Faiblesses |\n|------------|------|--------|------------|\n| Logistic Regression | Lineaire | Interpretable, baseline | Relations lineaires seulement |\n| Random Forest | Ensemble (Bagging) | Robuste, feature importance | Moins performant ici |\n| **Gradient Boosting** | Ensemble (Boosting) | **Meilleur AUC** | Plus lent |\n| SVM (RBF) | Kernel | Bon avec SMOTE | Lent, boite noire |\n| KNN | Instance | Simple | Sensible au scaling |\n\n---\n\n### 8.3 Metriques Finales\n\nLe modele final (Gradient Boosting optimise avec SMOTE) atteint:\n\n| Metrique | Score | Interpretation |\n|----------|-------|----------------|\n| Accuracy | ~75-80% | Performance globale correcte |\n| Precision | ~25-35% | 1 prediction positive sur 3-4 est correcte |\n| **Recall** | **~60-70%** | Detecte 60-70% des souscripteurs potentiels |\n| F1-Score | ~35-45% | Compromis precision/recall |\n| AUC-ROC | ~75-80% | Bonne capacite discriminante |\n\n---\n\n### 8.4 Strategie de Validation\n\n| Etape | Methode |\n|-------|---------|\n| Split initial | 80% train / 20% test (stratifie) |\n| Validation croisee | 5-Fold Stratified CV |\n| Optimisation | GridSearchCV avec scoring='recall' |\n| Evaluation finale | Test set (donnees jamais vues) |\n\n---\n\n### 8.5 Points Cles du Projet\n\n1. **Type de probleme**: Classification binaire supervisee (pas une regression)\n\n2. **Features**: 15 variables (demographiques, contact, historique) - `duration` exclue pour eviter le data leakage\n\n3. **Algorithmes**: 5 modeles de familles differentes (lineaire, kernel, bagging, boosting, instance)\n\n4. **Metriques**: Accuracy, Precision, Recall, F1-Score, AUC-ROC avec focus sur le **Recall**\n\n5. **Validation**: Train/test split stratifie + cross-validation 5-fold + GridSearchCV\n\n---\n\n### 8.6 Recommandations Business\n\n1. **Cibler les anciens souscripteurs**: `poutcome='success'` est le meilleur predicteur\n2. **Optimiser le timing**: Mars, septembre, octobre, decembre ont de meilleurs taux\n3. **Profils favorables**: Etudiants et retraites sont plus receptifs\n4. **Ajuster le seuil**: Baisser le seuil de decision pour maximiser le recall au detriment de la precision\n\n---\n\n### 8.7 Limites et Perspectives\n\n**Limites:**\n- Variable `duration` exclue reduit la performance predictive\n- Donnees de 2008-2010 (contexte economique different)\n- Desequilibre de classes important\n\n**Perspectives:**\n- Tester d'autres techniques de reechantillonnage (ADASYN, undersampling)\n- Feature engineering (interactions, agregations)\n- Modeles plus avances (XGBoost, LightGBM, reseaux de neurones)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume final des performances\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUME FINAL DU PROJET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: Bank Marketing (UCI)\")\n",
    "print(f\"Taille: 45,211 exemples\")\n",
    "print(f\"Probleme: Classification binaire\")\n",
    "print(f\"\\nMeilleur modele: Gradient Boosting\")\n",
    "print(f\"\\nPerformances finales:\")\n",
    "print(f\"  - Accuracy:  {accuracy_score(y_test_full, y_pred_final):.2%}\")\n",
    "print(f\"  - Precision: {precision_score(y_test_full, y_pred_final):.2%}\")\n",
    "print(f\"  - Recall:    {recall_score(y_test_full, y_pred_final):.2%}\")\n",
    "print(f\"  - F1-Score:  {f1_score(y_test_full, y_pred_final):.2%}\")\n",
    "print(f\"  - AUC-ROC:   {roc_auc_score(y_test_full, y_proba_final):.2%}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}